{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDXm4u_m0qXA"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oau-g4ESVzUA"
   },
   "source": [
    "## Recap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9w2QW-8_1wG"
   },
   "source": [
    "\n",
    "#### What have we learned till now?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bpyl_DL5Vvzi"
   },
   "source": [
    "<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/041/086/original/Screenshot_2023-07-31_at_10.27.26_AM.png?1690779929 width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odq7FYxdV0uU"
   },
   "source": [
    "- What are Decision Trees?\n",
    "- How to build Decision Trees?\n",
    "  - Splitting of nodes\n",
    "  - Entropy to measure impurity\n",
    "  - Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqo5KnK8UNlI"
   },
   "source": [
    "## Issue with Entropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ponEJDCqV9Mv"
   },
   "source": [
    "Do you recall how entropy is calculated?\n",
    "- $H(y) = - âˆ‘_{i=1}^k p(y_i)log(p(y_i))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pe87btXbV5Mg"
   },
   "source": [
    "<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/041/087/original/Screenshot_2023-07-31_at_10.27.33_AM.png?1690780005 width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4AWNTp89V-eA"
   },
   "source": [
    "\n",
    "\\\n",
    "So, what's the problem?\n",
    "- Notice that we take log of probability.\n",
    "- We have to do this for each feature, at each node.\n",
    "\n",
    "This is **computationally expensive** and **time consuming**.\n",
    "\n",
    "\\\n",
    "Do we have any alternatives?\n",
    "- That's when **Gini Impurity** comes into picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mOkUgKs0sj6"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b14fmC3h0u3P"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLvrymmrTmwL"
   },
   "source": [
    "### What is Gini Impurity?\n",
    "\n",
    "Similar to entropy, Gini is also\n",
    "- A measure of impurity of nodes.\n",
    "\n",
    "\\\n",
    "#### But how do we calculate Gini Impurity?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNwP9hFYWDoM"
   },
   "source": [
    "<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/041/088/original/Screenshot_2023-07-31_at_10.27.40_AM.png?1690780045 width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzFqffO0cz_j"
   },
   "source": [
    "Let's take an example to compare **Entropy vs. Gini Impurity**\n",
    "\n",
    "\\\n",
    "Suppose you have a binary classification problem.\n",
    "- $y ~ \\epsilon ~ \\{red, blue\\}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_rnJoMjWNgL"
   },
   "source": [
    "<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/041/089/original/Screenshot_2023-07-31_at_10.27.49_AM.png?1690780085 width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MG0Wvbl5N0bn"
   },
   "source": [
    "#### Visualizing Gini Impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIpsQNPIuAtR"
   },
   "source": [
    "Desmos plot: https://www.desmos.com/calculator/yhcwubphxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396
    },
    "id": "LOoCgCeprKSx",
    "outputId": "1922754f-df87-420f-f8ee-582d3f6e4365"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"700\"\n",
       "            height=\"375\"\n",
       "            src=\"https://www.desmos.com/calculator/yhcwubphxs\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f1482a5a110>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src=\"https://www.desmos.com/calculator/yhcwubphxs\", width=700, height=375)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WeqQuBaMWTIk"
   },
   "source": [
    "<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/041/090/original/Screenshot_2023-07-31_at_10.27.56_AM.png?1690780130 width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN9EbPApM1OW"
   },
   "source": [
    "```\n",
    "Quiz 1:\n",
    "\n",
    "A decision tree model uses Gini impurity as the splitting criterion.\n",
    "If a node has 60 instances of class A and 40 instances of class B,\n",
    "what is the Gini impurity at that node?\n",
    "\n",
    "A. 0.24\n",
    "B. 0.4\n",
    "C. 0.6\n",
    "D. 0.48\n",
    "\n",
    "Ans: D. 0.48\n",
    "```\n",
    "```\n",
    "Soln:\n",
    "Gini impurity = 1 - (p(A)^2 + p(B)^2)\n",
    "\n",
    "p(A) = 60 / 100 = 0.6\n",
    "p(B) = 40 / 100 = 0.4\n",
    "\n",
    "Gini impurity = 1 - (0.6^2 + 0.4^2)\n",
    "= 1 - (0.36 + 0.16)\n",
    "= 1 - 0.52\n",
    "= 0.48\n",
    "\n",
    "Therefore, the Gini impurity at that node is 0.48.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBnc1SoVail5"
   },
   "source": [
    "#### How do we calculate Information Gain for Gini Impurity?\n",
    "\n",
    "$IG = GI_{parent~node} - Weighted~GI_{child~nodes}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBzcFeh1sSd_"
   },
   "source": [
    "### Extra reading material\n",
    "\n",
    "Functions for calculating...\n",
    "- Gini impurity\n",
    "- Weighted gini impurity\n",
    "- Information Gain using gini impurity\n",
    "\n",
    "Link: https://colab.research.google.com/drive/1Cf6Bt6sQVYBbvnhtR3cPQENUO4DRGjMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSHxHCQLWmC-"
   },
   "source": [
    "<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/041/091/original/Screenshot_2023-07-31_at_10.28.04_AM.png?1690780165 width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0q8DBzI0x4H"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_wrwHyIOotR"
   },
   "source": [
    "## How to split on numerical features?\n",
    "\n",
    "I hope you remember that our data contains both,\n",
    "- Categorical features\n",
    "- Numerical features\n",
    "\n",
    "\\\n",
    "We already know that for Categorical feature,\n",
    "- DT splits data based on distinct categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9iyGbjY-Wr6s"
   },
   "source": [
    "<img src= https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/041/092/original/Screenshot_2023-07-31_at_10.28.12_AM.png?1690780208 width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zC1hYpFMPyWu"
   },
   "source": [
    "#### What happens when we have a Numerical feature?\n",
    "\n",
    "DT uses a **threshold-based approach**.\n",
    "\n",
    "  - Consider a numerical feature $f_1$ with $n$ different values.\n",
    "  - We select a threshold value (say $n_i$) that splits the data into two subsets.\n",
    "  - This is done on the basis of a condition such as:\n",
    "    - $f_1 < n_i$\n",
    "    - $f_1 > n_i$\n",
    "    - $f_1 = n_i$\n",
    "\n",
    "\\\n",
    "#### But how do we choose the threshold?\n",
    "\n",
    "The goal is to find the $threshold+condition$ that gives us **maximum information gain** after splitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jrc3qynYXCXV"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hvq9YfZLW3gT"
   },
   "source": [
    "<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/041/093/original/Screenshot_2023-07-31_at_10.28.20_AM.png?1690780248 width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0g9_zxiXDQW"
   },
   "source": [
    "\n",
    "1. Arrange the values in $f_1$ in increasing order\n",
    "2. Set each value of $f_1$ i.e. ($n_1$, $n_2$, $n_3$,..) as threshold\n",
    "3. Calculate the IG of the split for all values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEdkFvJDXFTZ"
   },
   "source": [
    "<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/041/094/original/Screenshot_2023-07-31_at_10.28.26_AM.png?1690780348 width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1-aYf1EXFiL"
   },
   "source": [
    "\n",
    "4. Which gives us $n$ IG values say IG$_1$, IG$_2$, IG$_3$ ... IG$_n$\n",
    "5. Choose whichever threshold gives the highest IG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kAgu9RBV19i"
   },
   "source": [
    "#### But what if two features yield equal IG?\n",
    "  - In such case,\n",
    "    - We can never know for sure which feature would be the best.\n",
    "    - So we can randomly select any feature to split the node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JIm6vzjF4Rp"
   },
   "source": [
    "#### Do you see any problem in splitting on numerical features?\n",
    "\n",
    "\\\n",
    "**Time Complexity**\n",
    "  - For a feature with 1000 different values,\n",
    "  - We have to compute 1000 different splits.\n",
    "  - This is very unefficient.\n",
    "\n",
    "\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPRub1IaXcQB"
   },
   "source": [
    "#### How can we make it more efficient?\n",
    "\n",
    "By binning the feature values.\n",
    "  - Suppose we have an `Age` column.\n",
    "    - Range: [20, 60]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKnZHcCDXdXq"
   },
   "source": [
    "<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/041/095/original/Screenshot_2023-07-31_at_10.28.37_AM.png?1690780522 width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbvGRQfAXdn5"
   },
   "source": [
    "\n",
    "  - We can bin the values into `Age Groups`.\n",
    "    - Bins: [20, 30), [30, 40), [40, 50), [50, 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dq7-9yag02Sg"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DY4UhX5YA48"
   },
   "source": [
    "## Overfit Vs Underfit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPIXEuNQYDLk"
   },
   "source": [
    "#### Recall in the last lecture we calculated the train & test accuracy for our DT -\n",
    "\n",
    "- Accuracy -\n",
    "  - Train: 1.0\n",
    "  - Test: 0.76\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FleZP1PKX915"
   },
   "source": [
    "<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/041/096/original/Screenshot_2023-07-31_at_10.28.44_AM.png?1690780545 width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KFITXUUYE5N"
   },
   "source": [
    "\\\n",
    "#### What can we conclude from this?\n",
    "\n",
    "- There's a big difference in training & test accuracy.\n",
    "- It means, the DT model is **overfitting** the data.\n",
    "\n",
    "\\\n",
    "#### When do you think a DT will **overfit**?\n",
    "\n",
    "If all the leaf nodes are pure then,\n",
    "- the DT would have 100% accuracy on training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYPHXV2XYPw-"
   },
   "source": [
    "\\\n",
    "#### Why does DT overfit as the depth increases?\n",
    "\n",
    "Imagine we have 1000 data points.\n",
    "- as the depth increases, the data set becomes smaller.\n",
    "\n",
    "By the time we reach the leaf node,\n",
    "- there will be fewer points.\n",
    "- these points coule be noise/outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bpe9nR5YJ3r"
   },
   "source": [
    "<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/041/097/original/Screenshot_2023-07-31_at_10.28.51_AM.png?1690780598 width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgABfTXDWUzf"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This results in **Overfitting**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qSGLqTQxXK-"
   },
   "source": [
    "#### How can we resolve this problem?\n",
    "  \n",
    "  - Stop splitting nodes before they become pure.\n",
    "  - Or in simpler terms, by reducing the **Depth of the DT**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncN1il-6ZuMH"
   },
   "source": [
    "#### But what if the depth is too low?\n",
    "\n",
    "\\\n",
    "Typically, we let the tree grow until we get a pure node.\n",
    "\n",
    "What if we stop the tree growth at a depth=2?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWhU4a4s4ZSa"
   },
   "source": [
    "This would mean...\n",
    "- Lot of -ve points might also fall in that leaf node,\n",
    "- and they'll be incorrectly classified as +ve.\n",
    "\n",
    "\\\n",
    "This results in **Underfitting**.\n",
    "\n",
    "\\\n",
    "If the tree depth is less,\n",
    "- it means the splits are less.\n",
    "\n",
    "Hence, **number of hyper-planes dividing the data space are less** </br> i.e., less variance and more bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUMiom9bCzoU"
   },
   "source": [
    "**Conclusion:**\n",
    "\n",
    "- A **deep tree** usually Overfits the training data.\n",
    "- A **shallow tree** usually Underfits the training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7nm-JEHYold"
   },
   "source": [
    "<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/041/098/original/Screenshot_2023-07-31_at_10.28.58_AM.png?1690780751 width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SM5p2GUZFPRB"
   },
   "source": [
    "### Tradeoff\n",
    "\n",
    "#### How do we find the right depth of our DT?\n",
    "\n",
    "Our goal is to -\n",
    "- not overfit outliers\n",
    "- not underfit training samples\n",
    "\n",
    "\\\n",
    "The idea here is to -\n",
    "- treat **depth** as a hyperparameter\n",
    "- and find its optimal value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asHxBu2F08c7"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
